# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d_Qy7kHrwns9wKnjz57A8DXSst8i63eA
"""

from nltk.stem.snowball import SnowballStemmer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import classification_report
import re
from sklearn import preprocessing
import numpy as np
from sklearn import svm
import pandas as pd

stemmer = SnowballStemmer("romanian")

def citire(path):
    with open(path, mode="r", encoding="utf-8") as f:
        x = f.readlines()
    return x

def faraSemne(string):
    return string.isalpha()

def preprocess_string(propozitie):
    prop_fara_snes = str(propozitie).replace("$NE$",'')
    prop_impartita = prop_fara_snes.split(' ')
    prop_fara_semne = [''.join(filter(faraSemne,x)) for x in list(prop_impartita)]
    array_final = [x.lower() for x in prop_fara_semne if x!='']
    propozitie = ' '.join(x for x in array_final)
    return propozitie

def betterCount(word,string):
    count = sum(1 for _ in re.finditer(r'\b%s\b' % re.escape(word), string))
    return count

train_samples = citire('train_samples.txt')
train_labels = citire('train_labels.txt')
validation_source_samples = citire('validation_source_samples.txt')
validation_source_labels = citire('validation_source_labels.txt')
validation_target_samples = citire('validation_target_samples.txt')
validation_target_labels = citire('validation_target_labels.txt')
test_samples = citire('test_samples.txt')

train_labels = [train_labels[x].split('\t')[1].replace("\n",'') for x in range(len(train_labels))]
validation_source_labels = [validation_source_labels[x].split('\t')[1].replace("\n",'') for x in range(len(validation_source_labels))]
validation_target_labels = [validation_target_labels[x].split('\t')[1].replace("\n",'') for x in range(len(validation_target_labels))]

prop = [train_samples[x].split('\t')[1] for x in range(len(train_samples))]
validation_source_samples = [validation_source_samples[x].split('\t')[1] for x in range(len(validation_source_samples))]
validation_target_samples = [validation_target_samples[x].split('\t')[1] for x in range(len(validation_target_samples))]
iduri = [x for x in test_samples]
test_samples = [test_samples[x].split('\t')[1] for x in range(len(test_samples))]
iduri = [iduri[x].split('\t')[0] for x in range(len(iduri))]
print(iduri)

for i in range(len(prop)):
    prop[i] = preprocess_string(prop[i])
for i in range(len(validation_source_samples)):
    validation_source_samples[i] = preprocess_string(validation_source_samples[i])
for i in range(len(validation_target_samples)):
    validation_target_samples[i] = preprocess_string(validation_target_samples[i])
for i in range(len(test_samples)):
    test_samples[i] = preprocess_string(test_samples[i])

print("ok")
vectorizer = CountVectorizer(max_features = 30000)
vectorizer.fit(prop)
print(len(vectorizer.vocabulary_))
print(vectorizer.vocabulary_)

train_vectors = vectorizer.transform(prop).toarray()
source_vectors = vectorizer.transform(validation_source_samples).toarray()
target_vectors = vectorizer.transform(validation_target_samples).toarray()
test_vectors = vectorizer.transform(test_samples).toarray()

scaler = preprocessing.StandardScaler()
scaler.fit(train_vectors)
scaler.fit(source_vectors)
scaler.fit(target_vectors)
scaler.fit(test_vectors)

train_vectors = scaler.transform(train_vectors)
source_vectors = scaler.transform(source_vectors)
target_vectors = scaler.transform(target_vectors)
test_vectors = scaler.transform(test_vectors)

classifier = svm.LinearSVC(C = 1)
classifier.fit(train_vectors, train_labels)
classifier.fit(source_vectors,validation_source_labels)
classifier.fit(target_vectors,validation_target_labels)

predictions = classifier.predict(test_vectors)

output = pd.DataFrame( data={"id":iduri, "label":predictions} )
output.to_csv("results.csv",index=False)